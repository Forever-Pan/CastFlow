#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
SFTè®­ç»ƒè„šæœ¬
åŸºäºCSVæ•°æ®é›†è¿›è¡Œç›‘ç£å¼å¾®è°ƒè®­ç»ƒï¼Œæ”¯æŒQwen3-4Bç­‰æ¨¡å‹

ä½¿ç”¨æ–¹æ³•:
    # åŸºæœ¬ä½¿ç”¨ï¼ˆä½¿ç”¨é»˜è®¤å‚æ•°ï¼‰
    python sft_train.py
    
    # æŒ‡å®šæ•°æ®é›†ã€æ¨¡å‹å’Œè¾“å‡ºè·¯å¾„
    python sft_train.py \
        --dataset_path ./datasets/SFT_RL_bank/ETTH1/grok_final.csv \
        --model_path ./models/Qwen3-8B \
        --output_dir ./models/ETTH1/sft_Qwen3_8B
    
    # å¤šGPUè®­ç»ƒ
    torchrun --nproc_per_node=1 --master_port=32588 sft_train.py \
     --model_path ./models/Qwen3-0.6B \
     --dataset_path ./datasets/SFT_RL_bank/sunny/grok.csv \
     --output_dir ./models/sunny/sft_qwen3_4b
"""

import os
import sys
import argparse
import pandas as pd
import torch
from dataclasses import dataclass
from typing import List, Dict, Any
from datasets import Dataset
from transformers import (
    AutoTokenizer, 
    AutoModelForCausalLM,
    TrainingArguments,
    Trainer
)

# å°è¯•å¯¼å…¥PEFTï¼ˆç”¨äºLoRAï¼‰
try:
    from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType
    HAS_PEFT = True
except ImportError:
    HAS_PEFT = False
    print("âš ï¸  è­¦å‘Š: æœªå®‰è£…peftåº“ï¼ŒLoRAåŠŸèƒ½å°†ä¸å¯ç”¨ã€‚å®‰è£…å‘½ä»¤: pip install peft")
from pathlib import Path
import shutil

# ä¿®å¤CUDA_HOMEç¯å¢ƒå˜é‡ï¼ˆé¿å…DeepSpeedå¯¼å…¥é”™è¯¯ï¼‰
# æ£€æŸ¥å¹¶è®¾ç½®æ­£ç¡®çš„CUDA_HOMEè·¯å¾„
nvcc_path = shutil.which("nvcc")
if nvcc_path:
    cuda_home = str(Path(nvcc_path).parent.parent)
    os.environ["CUDA_HOME"] = cuda_home
else:
    # å¦‚æœnvccä¸åœ¨PATHä¸­ï¼Œå°è¯•å¸¸è§çš„CUDAè·¯å¾„
    common_cuda_paths = [
        "/usr/local/cuda",
        "/usr/local/cuda-12.6",
        "/usr/local/cuda-12",
        "/usr/local/cuda-11.8",
    ]
    for cuda_path in common_cuda_paths:
        nvcc_file = Path(cuda_path) / "bin" / "nvcc"
        if nvcc_file.exists():
            os.environ["CUDA_HOME"] = cuda_path
            # åŒæ—¶æ·»åŠ åˆ°PATHä»¥ä¾¿åç»­ä½¿ç”¨
            os.environ["PATH"] = f"{cuda_path}/bin:{os.environ.get('PATH', '')}"
            break

# æ¸…ç†CUDA_HOMEä¸­çš„é”™è¯¯æ ¼å¼ï¼ˆç§»é™¤å¼€å¤´çš„å†’å·ç­‰ï¼‰
if "CUDA_HOME" in os.environ:
    cuda_home = os.environ["CUDA_HOME"].strip()
    # ç§»é™¤å¼€å¤´çš„å†’å·
    if cuda_home.startswith(":"):
        cuda_home = cuda_home[1:]
    # å¦‚æœæœ‰å¤šä¸ªè·¯å¾„ï¼ˆç”¨å†’å·åˆ†éš”ï¼‰ï¼Œå–ç¬¬ä¸€ä¸ªæœ‰æ•ˆè·¯å¾„
    if ":" in cuda_home:
        cuda_home = cuda_home.split(":")[0]
    os.environ["CUDA_HOME"] = cuda_home
# ============ 1. è§£æå‘½ä»¤è¡Œå‚æ•° ============ 
parser = argparse.ArgumentParser(
    description='SFTè®­ç»ƒè„šæœ¬ - æ”¯æŒQwen3-4Bç­‰æ¨¡å‹çš„ç›‘ç£å¼å¾®è°ƒ',
    formatter_class=argparse.ArgumentDefaultsHelpFormatter
)

# æ ¸å¿ƒå‚æ•°ï¼šæ•°æ®é›†ã€æ¨¡å‹è·¯å¾„ã€è¾“å‡ºç›®å½•
parser.add_argument(
    '--dataset_path', 
    type=str, 
    default='./datasets/SFT_RL_bank/grok_merge.csv',
    help='è®­ç»ƒæ•°æ®é›†CSVæ–‡ä»¶è·¯å¾„ï¼ˆå¿…é¡»åŒ…å«promptå’Œresponseåˆ—ï¼‰'
)
parser.add_argument(
    '--model_path', 
    type=str, 
    default='./models/Qwen3-4B',
    help='æœ¬åœ°LLMæ¨¡å‹è·¯å¾„ï¼ˆæ”¯æŒQwen3-4Bç­‰æ¨¡å‹ï¼‰'
)
parser.add_argument(
    '--output_dir', 
    type=str, 
    default='./models/merge/sft_qwen3_0.6B',
    help='è®­ç»ƒåæ¨¡å‹çš„ä¿å­˜ç›®å½•'
)

# è®­ç»ƒè¶…å‚æ•°
parser.add_argument('--batch_size', type=int, default=1, help='æ¯è®¾å¤‡batch sizeï¼ˆQwen3-4Bå»ºè®®ï¼šA800=2-4ï¼Œ4090=1-2ï¼‰')
parser.add_argument('--gradient_accumulation', type=int, default=8, help='æ¢¯åº¦ç´¯ç§¯æ­¥æ•°ï¼ˆQwen3-4Bå»ºè®®ï¼šA800=2-4ï¼Œ4090=4-8ï¼‰')
parser.add_argument('--learning_rate', type=float, default=1e-5, help='å­¦ä¹ ç‡ï¼ˆLoRAå»ºè®®1e-4ï¼Œå…¨é‡å¾®è°ƒå»ºè®®5e-5ï¼‰')
parser.add_argument('--num_epochs', type=int, default=2, help='è®­ç»ƒè½®æ•°ï¼ˆå»ºè®®3-5è½®ï¼‰')
parser.add_argument('--max_length', type=int, default=14000, help='æœ€å¤§åºåˆ—é•¿åº¦ï¼ˆQwen3-4Bæ”¯æŒ8192ï¼‰')
parser.add_argument('--save_strategy', type=str, default='steps', choices=['no', 'steps', 'epoch'], help='æ¨¡å‹ä¿å­˜ç­–ç•¥')
parser.add_argument('--save_steps', type=int, default=275, help='æŒ‰stepsä¿å­˜æ—¶çš„æ­¥æ•°ï¼ˆä»…å½“save_strategy=stepsæ—¶ç”Ÿæ•ˆï¼‰')
parser.add_argument('--save_total_limit', type=int, default=5, help='æœ€å¤šä¿ç•™çš„checkpointæ•°é‡')
parser.add_argument('--use_deepspeed', action='store_true', help='æ˜¯å¦ä½¿ç”¨DeepSpeedï¼ˆé»˜è®¤å¼€å¯ï¼Œé€‚åˆå¤§æ¨¡å‹ï¼‰')
parser.add_argument('--no_deepspeed', dest='use_deepspeed', action='store_false', help='ç¦ç”¨DeepSpeed')
parser.set_defaults(use_deepspeed=True)  # è®¾ç½®é»˜è®¤å€¼ä¸ºTrue
parser.add_argument('--use_lora', action='store_true', help='æ˜¯å¦ä½¿ç”¨LoRAï¼ˆé»˜è®¤å…³é—­ï¼Œå¯æ˜¾å¼æŒ‡å®šå¼€å¯ï¼‰')
parser.add_argument('--lora_r', type=int, default=16, help='LoRA rankï¼ˆé»˜è®¤16ï¼Œå¯è°ƒ8/32/64ï¼‰')
parser.add_argument('--lora_alpha', type=int, default=32, help='LoRA alphaï¼ˆé»˜è®¤32ï¼Œé€šå¸¸ä¸ºrankçš„2å€ï¼‰')
parser.add_argument('--lora_dropout', type=float, default=0.05, help='LoRA dropoutï¼ˆé»˜è®¤0.05ï¼‰')
parser.add_argument('--lora_target_modules', type=str, default="", help='LoRAç›®æ ‡æ¨¡å—ï¼Œé€—å·åˆ†éš”ï¼ˆé»˜è®¤è‡ªåŠ¨æ£€æµ‹Qwenæ¨¡å‹ï¼‰')

args = parser.parse_args()

# æ ¸å¿ƒå‚æ•°
DATASET_PATH = args.dataset_path
MODEL_PATH = args.model_path
OUTPUT_DIR = args.output_dir

# éªŒè¯æ ¸å¿ƒå‚æ•°
if not os.path.exists(DATASET_PATH):
    print(f"âŒ é”™è¯¯: æ•°æ®é›†æ–‡ä»¶ä¸å­˜åœ¨: {DATASET_PATH}")
    sys.exit(1)

if not os.path.exists(MODEL_PATH):
    print(f"âŒ é”™è¯¯: æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {MODEL_PATH}")
    print(f"   è¯·ç¡®ä¿æ¨¡å‹å·²ä¸‹è½½åˆ°æŒ‡å®šè·¯å¾„ï¼Œæˆ–ä½¿ç”¨æ­£ç¡®çš„æ¨¡å‹è·¯å¾„")
    sys.exit(1)

print("=" * 80)
print("SFTè®­ç»ƒé…ç½®")
print("=" * 80)
print(f"  æ•°æ®é›†è·¯å¾„: {DATASET_PATH}")
print(f"  æ¨¡å‹è·¯å¾„: {MODEL_PATH}")
print(f"  è¾“å‡ºç›®å½•: {OUTPUT_DIR}")
print("=" * 80)
# è®­ç»ƒè¶…å‚æ•°
BATCH_SIZE = args.batch_size
GRADIENT_ACCUMULATION = args.gradient_accumulation
LEARNING_RATE = args.learning_rate
NUM_EPOCHS = args.num_epochs
MAX_LENGTH = args.max_length
SAVE_STRATEGY = args.save_strategy
SAVE_STEPS = args.save_steps
SAVE_TOTAL_LIMIT = args.save_total_limit
USE_DEEPSPEED = args.use_deepspeed
USE_LORA = args.use_lora
LORA_R = args.lora_r
LORA_ALPHA = args.lora_alpha
LORA_DROPOUT = args.lora_dropout
LORA_TARGET_MODULES = args.lora_target_modules.split(',') if args.lora_target_modules else None

# ============ 2. DeepSpeed é…ç½®ï¼ˆå¯é€‰ï¼‰ ============ 
ds_config = None
if USE_DEEPSPEED:
    try:
        import deepspeed
        ds_config = {
            "fp16": {"enabled": False},
            "bf16": {"enabled": True},
            "zero_optimization": {
                "stage": 2,                      # ZeRO-2 é€‚åˆ 8B æ¨¡å‹åœ¨ 4x80G è·‘å…¨é‡
                "allgather_partitions": True,
                "allgather_bucket_size": 2e8,
                "overlap_comm": True,
                "reduce_scatter": True,
                "reduce_bucket_size": 2e8,
                "contiguous_gradients": True
            },
            "gradient_accumulation_steps": "auto",
            "gradient_clipping": "auto",
            "steps_per_print": 10,
            "train_batch_size": "auto",
            "train_micro_batch_size_per_gpu": "auto",
            "wall_clock_breakdown": False
        }
        print("âœ… å·²å¯ç”¨DeepSpeedä¼˜åŒ–")
    except ImportError:
        print("âš ï¸  è­¦å‘Š: æœªå®‰è£…deepspeedï¼Œå°†ä¸ä½¿ç”¨DeepSpeedä¼˜åŒ–")
        USE_DEEPSPEED = False
        ds_config = None
else:
    print("â„¹ï¸  æœªå¯ç”¨DeepSpeedï¼Œä½¿ç”¨æ ‡å‡†è®­ç»ƒæ¨¡å¼")

# ============ 3. åŠ è½½åˆ†è¯å™¨ ============ 
print(f"\næ­£åœ¨åŠ è½½åˆ†è¯å™¨: {MODEL_PATH}")
tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right"
print(f"âœ… åˆ†è¯å™¨åŠ è½½å®Œæˆï¼Œè¯æ±‡è¡¨å¤§å°: {len(tokenizer)}")

# ============ 4. ä»CSVåŠ è½½æ•°æ®å¹¶è½¬æ¢ä¸ºmessagesæ ¼å¼ ============ 
print(f"\næ­£åœ¨åŠ è½½æ•°æ®é›†: {DATASET_PATH}")
df = pd.read_csv(DATASET_PATH, keep_default_na=False)
print(f"  åŸå§‹æ•°æ®è¡Œæ•°: {len(df)}")

# æ£€æŸ¥å¿…éœ€çš„åˆ—
required_columns = ['prompt', 'response']
missing_columns = [col for col in required_columns if col not in df.columns]
if missing_columns:
    print(f"âŒ é”™è¯¯: æ•°æ®é›†ç¼ºå°‘å¿…éœ€çš„åˆ—: {missing_columns}")
    print(f"   æ•°æ®é›†åŒ…å«çš„åˆ—: {list(df.columns)}")
    sys.exit(1)

# è¿‡æ»¤æ‰responseä¸ºç©ºçš„è¡Œ
df = df[df['response'].str.strip() != '']
print(f"  æœ‰æ•ˆæ•°æ®è¡Œæ•°: {len(df)}")

# è½¬æ¢ä¸ºmessagesæ ¼å¼
def csv_to_messages(row):
    """å°†CSVè¡Œè½¬æ¢ä¸ºchat messagesæ ¼å¼"""
    prompt = str(row['prompt']).strip()
    response = str(row['response']).strip()
    
    # ç¡®ä¿promptå’Œresponseéƒ½ä¸ä¸ºç©º
    if not prompt or not response:
        return None
    
    messages = [
        {"role": "user", "content": prompt},
        {"role": "assistant", "content": response}
    ]
    return {"messages": messages}

# è½¬æ¢ä¸ºDataset
data_list = []
for idx, row in df.iterrows():
    try:
        messages = csv_to_messages(row)
        if messages is not None:
            data_list.append(messages)
    except Exception as e:
        if idx < 10:  # åªæ˜¾ç¤ºå‰10ä¸ªé”™è¯¯çš„è¯¦ç»†ä¿¡æ¯
            print(f"  è­¦å‘Š: è·³è¿‡ç¬¬{idx}è¡Œæ•°æ®ï¼Œé”™è¯¯: {str(e)}")
        continue

if len(data_list) == 0:
    print("âŒ é”™è¯¯: æ²¡æœ‰æœ‰æ•ˆçš„æ•°æ®å¯ä»¥è®­ç»ƒ")
    sys.exit(1)

print(f"âœ… æˆåŠŸè½¬æ¢ {len(data_list)} æ¡æ•°æ®")
dataset = Dataset.from_list(data_list)

# ============ 5. åˆ†è¯ä¸ Mask é€»è¾‘ ============ 
def tokenize_multiturn_chat(example):
    """å¯¹å¤šè½®å¯¹è¯è¿›è¡Œåˆ†è¯å’Œæ ‡ç­¾mask"""
    messages = example["messages"]
    full_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)
    full_tokenized = tokenizer(full_text, truncation=True, max_length=MAX_LENGTH, padding=False, add_special_tokens=False)
    
    input_ids = list(full_tokenized["input_ids"])
    labels = list(input_ids)
    
    # åªå¯¹assistantçš„å›å¤éƒ¨åˆ†è®¡ç®—lossï¼Œå…¶ä»–éƒ¨åˆ†maskæ‰
    prev_len = 0
    for i in range(1, len(messages) + 1):
        curr_all_text = tokenizer.apply_chat_template(messages[:i], tokenize=False, add_generation_prompt=False)
        curr_tokenized = tokenizer(curr_all_text, add_special_tokens=False)
        curr_len = len(curr_tokenized["input_ids"])
        
        is_assistant = (messages[i-1]["role"] == "assistant")
        is_last_message = (i == len(messages))
        
        # åªä¿ç•™æœ€åä¸€ä¸ªassistantæ¶ˆæ¯çš„lossï¼Œå…¶ä»–éƒ½mask
        if not (is_assistant and is_last_message):
            start_idx = prev_len
            end_idx = min(curr_len, len(labels))
            if start_idx < end_idx:
                for idx in range(start_idx, end_idx):
                    labels[idx] = -100
        prev_len = curr_len
        if prev_len >= len(labels):
            break
                
    return {"input_ids": input_ids, "attention_mask": full_tokenized["attention_mask"], "labels": labels}

print("\næ­£åœ¨å¯¹æ•°æ®è¿›è¡Œåˆ†è¯...")
tokenized_dataset = dataset.map(
    tokenize_multiturn_chat, 
    remove_columns=dataset.column_names, 
    num_proc=min(8, os.cpu_count())
)
print(f"âœ… åˆ†è¯å®Œæˆï¼Œæ•°æ®é›†å¤§å°: {len(tokenized_dataset)}")

# ============ 6. åŠ è½½æ¨¡å‹å¹¶åº”ç”¨LoRAï¼ˆå¦‚æœå¯ç”¨ï¼‰ ============ 
print(f"\næ­£åœ¨åŠ è½½æ¨¡å‹: {MODEL_PATH}")
print("  è¿™å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´ï¼Œè¯·è€å¿ƒç­‰å¾…...")
model = AutoModelForCausalLM.from_pretrained(
    MODEL_PATH,
    dtype=torch.bfloat16,
    trust_remote_code=True,
    torch_dtype=torch.bfloat16,
)
print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ")

# åº”ç”¨LoRAé…ç½®
if USE_LORA:
    if not HAS_PEFT:
        print("âŒ é”™è¯¯: æœªå®‰è£…peftåº“ï¼Œæ— æ³•ä½¿ç”¨LoRAã€‚è¯·è¿è¡Œ: pip install peft")
        sys.exit(1)
    
    # è‡ªåŠ¨æ£€æµ‹ç›®æ ‡æ¨¡å—ï¼ˆå¦‚æœæœªæŒ‡å®šï¼‰
    if LORA_TARGET_MODULES is None:
        # æ ¹æ®æ¨¡å‹ç±»å‹è‡ªåŠ¨é€‰æ‹©ç›®æ ‡æ¨¡å—
        model_type = getattr(model.config, 'model_type', '').lower()
        model_name_lower = MODEL_PATH.lower()
        
        if 'qwen' in model_type or 'qwen' in model_name_lower:
            # Qwen3æ¨¡å‹ä½¿ç”¨è¿™äº›æ¨¡å—
            LORA_TARGET_MODULES = ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
            print(f"  æ£€æµ‹åˆ°Qwenæ¨¡å‹ï¼Œä½¿ç”¨Qwenä¸“ç”¨LoRAç›®æ ‡æ¨¡å—")
        elif 'llama' in model_type or 'llama' in model_name_lower:
            LORA_TARGET_MODULES = ["q_proj", "k_proj", "v_proj", "o_proj"]
            print(f"  æ£€æµ‹åˆ°LLaMAæ¨¡å‹ï¼Œä½¿ç”¨LLaMAä¸“ç”¨LoRAç›®æ ‡æ¨¡å—")
        else:
            # é€šç”¨é…ç½®ï¼Œå°è¯•å¸¸è§çš„attentionæ¨¡å—
            LORA_TARGET_MODULES = ["q_proj", "k_proj", "v_proj", "o_proj"]
            print(f"âš ï¸  è­¦å‘Š: æœªè¯†åˆ«çš„æ¨¡å‹ç±»å‹ {model_type}ï¼Œä½¿ç”¨é»˜è®¤LoRAç›®æ ‡æ¨¡å—")
    
    print(f"âœ… å¯ç”¨LoRAé…ç½®:")
    print(f"   Rank (r): {LORA_R}")
    print(f"   Alpha: {LORA_ALPHA}")
    print(f"   Dropout: {LORA_DROPOUT}")
    print(f"   ç›®æ ‡æ¨¡å—: {LORA_TARGET_MODULES}")
    
    lora_config = LoraConfig(
        r=LORA_R,
        lora_alpha=LORA_ALPHA,
        target_modules=LORA_TARGET_MODULES,
        lora_dropout=LORA_DROPOUT,
        bias="none",
        task_type=TaskType.CAUSAL_LM,
    )
    
    model = get_peft_model(model, lora_config)
    model.print_trainable_parameters()
else:
    print("â„¹ï¸  æœªå¯ç”¨LoRAï¼Œä½¿ç”¨å…¨é‡å¾®è°ƒ")

# ============ 7. Data Collator ============ 
@dataclass
class ToolDataCollator:
    tokenizer: Any
    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:
        input_ids = [torch.tensor(f["input_ids"]) for f in features]
        labels = [torch.tensor(f["labels"]) for f in features]
        attention_mask = [torch.tensor(f["attention_mask"]) for f in features]
        
        input_ids = torch.nn.utils.rnn.pad_sequence(input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id)
        labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=-100)
        attention_mask = torch.nn.utils.rnn.pad_sequence(attention_mask, batch_first=True, padding_value=0)
        
        return {"input_ids": input_ids, "labels": labels, "attention_mask": attention_mask}

# ============ 8. è®­ç»ƒé…ç½® ============ 
training_args_dict = {
    "output_dir": OUTPUT_DIR,
    "num_train_epochs": NUM_EPOCHS,
    "per_device_train_batch_size": BATCH_SIZE,
    "gradient_accumulation_steps": GRADIENT_ACCUMULATION,
    "learning_rate": LEARNING_RATE,
    "warmup_ratio": 0.03,
    "lr_scheduler_type": "cosine",
    "bf16": True,
    "logging_steps": 1,
    "save_strategy": SAVE_STRATEGY,
    "save_steps": SAVE_STEPS,
    "save_total_limit": SAVE_TOTAL_LIMIT,
    "gradient_checkpointing": True,
    "gradient_checkpointing_kwargs": {"use_reentrant": False},
    "remove_unused_columns": False,
    "ddp_find_unused_parameters": False
}

# åªåœ¨å¯ç”¨DeepSpeedæ—¶æ·»åŠ deepspeedé…ç½®
if USE_DEEPSPEED and ds_config is not None:
    training_args_dict["deepspeed"] = ds_config

training_args = TrainingArguments(**training_args_dict)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    data_collator=ToolDataCollator(tokenizer)
)

# åˆ›å»ºè¾“å‡ºç›®å½•
os.makedirs(OUTPUT_DIR, exist_ok=True)

print("\n" + "=" * 80)
print("ğŸš€ å¯åŠ¨SFTè®­ç»ƒ")
print("=" * 80)
print(f"  æ¨¡å‹è·¯å¾„: {MODEL_PATH}")
print(f"  æ•°æ®é›†: {DATASET_PATH} ({len(tokenized_dataset)} æ¡æ ·æœ¬)")
print(f"  è¾“å‡ºç›®å½•: {OUTPUT_DIR}")
print(f"  è®­ç»ƒé…ç½®:")
print(f"    - Batch Size: {BATCH_SIZE}")
print(f"    - Gradient Accumulation: {GRADIENT_ACCUMULATION}")
print(f"    - Effective Batch Size: {BATCH_SIZE * GRADIENT_ACCUMULATION * torch.cuda.device_count() if torch.cuda.is_available() else BATCH_SIZE * GRADIENT_ACCUMULATION}")
print(f"    - Learning Rate: {LEARNING_RATE}")
print(f"    - Epochs: {NUM_EPOCHS}")
print(f"    - Max Length: {MAX_LENGTH}")
print(f"    - DeepSpeed: {'âœ… å¯ç”¨' if USE_DEEPSPEED and ds_config else 'âŒ æœªå¯ç”¨'}")
print(f"    - LoRA: {'âœ… å¯ç”¨' if USE_LORA else 'âŒ æœªå¯ç”¨'}")
if USE_LORA:
    print(f"      * Rank: {LORA_R}, Alpha: {LORA_ALPHA}, Dropout: {LORA_DROPOUT}")
    if LORA_TARGET_MODULES:
        print(f"      * ç›®æ ‡æ¨¡å—: {', '.join(LORA_TARGET_MODULES)}")
print("=" * 80)
print()

trainer.train()

# ============ 9. ä¿å­˜æ¨¡å‹ ============ 
if trainer.is_world_process_zero() or not torch.distributed.is_initialized():
    print("\n" + "=" * 80)
    print("ğŸ’¾ ä¿å­˜è®­ç»ƒåçš„æ¨¡å‹")
    print("=" * 80)
    
    if USE_LORA:
        # LoRAæ¨¡å¼ï¼šåªä¿å­˜adapteræƒé‡
        model.save_pretrained(OUTPUT_DIR)
        tokenizer.save_pretrained(OUTPUT_DIR)
        print(f"âœ… SFTè®­ç»ƒå®Œæˆï¼")
        print(f"   LoRA adapterå·²ä¿å­˜è‡³: {OUTPUT_DIR}")
        print(f"   âš ï¸  æ³¨æ„: è¿™æ˜¯LoRA adapteræƒé‡ï¼Œä½¿ç”¨æ—¶éœ€è¦:")
        print(f"      1. åŠ è½½åŸºç¡€æ¨¡å‹: {MODEL_PATH}")
        print(f"      2. åŠ è½½adapter: {OUTPUT_DIR}")
    else:
        # å…¨é‡å¾®è°ƒï¼šä¿å­˜å®Œæ•´æƒé‡
        trainer.save_model(OUTPUT_DIR)
        tokenizer.save_pretrained(OUTPUT_DIR)
        print(f"âœ… SFTè®­ç»ƒå®Œæˆï¼")
        print(f"   å®Œæ•´æ¨¡å‹å·²ä¿å­˜è‡³: {OUTPUT_DIR}")
        print(f"   å¯ä»¥ç›´æ¥ä½¿ç”¨è¯¥è·¯å¾„åŠ è½½æ¨¡å‹")
    
    print("=" * 80)

